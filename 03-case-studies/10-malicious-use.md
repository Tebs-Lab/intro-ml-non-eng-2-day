# Malicious Use

This one isn't exactly an "error" ... but it is a problem. Generative AI is somewhat more "general" than previous kinds of models, which means it can be used for a lot of "off label" or unanticipated purposes. Here's a short list of problematic things that GenAI has been used to do:

* Create fake pornographic images of Taylor Swift (and countless others) [https://www.404media.co/taylor-swift-deepfakes-ai-generated-porn/](https://www.404media.co/taylor-swift-deepfakes-ai-generated-porn/)

* Create falsified audio of Joe Biden asking people not to vote [https://apnews.com/article/biden-robocalls-artificial-intelligence-new-hampshire-texas-a8665277d43d05380d2c7594edf27617]
(https://apnews.com/article/biden-robocalls-artificial-intelligence-new-hampshire-texas-a8665277d43d05380d2c7594edf27617)

* Create garbage tabloid websites that have great SEO and generate ad revenue [https://www.technologyreview.com/2023/04/04/1070938/we-are-hurtling-toward-a-glitchy-spammy-scammy-ai-powered-internet/](https://www.technologyreview.com/2023/04/04/1070938/we-are-hurtling-toward-a-glitchy-spammy-scammy-ai-powered-internet/)

* Automate Phishing and Malware [https://krebsonsecurity.com/2023/08/meet-the-brains-behind-the-malware-friendly-ai-chat-service-wormgpt/](https://krebsonsecurity.com/2023/08/meet-the-brains-behind-the-malware-friendly-ai-chat-service-wormgpt/)

## Additional Questions

* Are you familiar with any other malicious uses?
    * Are there any that you're particularly worried about?
* Do model providers have an obligation to prevent malicious uses?
* To what extent can AI firms prevent their products from being used maliciously?
    * What tactics could they use to stop some of the above examples?