# Hallucination

Sometimes generative models make stuff up. "Hallucination" has become the term of art used to describe this phenomenon. Sometimes hallucinations are a big problem, here's a quote from a lawsuit filed by The New York Times vs Microsoft and OpenAI 

* Selected documents from the case [https://www.bakerlaw.com/new-york-times-v-microsoft/](https://www.bakerlaw.com/new-york-times-v-microsoft/)
* The complaint [https://admin.bakerlaw.com/wp-content/uploads/2024/01/ECF-1-Complaint-1-1.pdf](https://admin.bakerlaw.com/wp-content/uploads/2024/01/ECF-1-Complaint-1-1.pdf)

> Defendants are aware that their GPT-based products produce inaccurate content that is falsely attributed to The Times and yet continue to profit commercially from creating and attributing inaccurate content to The Times. As such, Defendants have intentionally violated 15 U.S.C ยง 1125(c).

Here, the model cited non-existent NYTimes journalism as a source for fictitious information, and they got sued.

Hallucination also occurs, for example, when language models produce incorrect results to math problems [https://arstechnica.com/science/2023/05/when-it-comes-to-advanced-math-chatgpt-is-no-star-student/](https://arstechnica.com/science/2023/05/when-it-comes-to-advanced-math-chatgpt-is-no-star-student/). 

There are countless examples.

## Additional Questions:

* Are you aware of any other examples of generative models "hallucinating"?
* Why do you think hallucinations occur?
    * If the training data consumed were 100% factual and accurate, would hallucination still occur?
* Can you think of any situations where "hallucination" is the desired outcome?
* How might you decrease or eliminate hallucinations?
